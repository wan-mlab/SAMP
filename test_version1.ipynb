{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict,LeaveOneOut,KFold\n",
    "from sklearn.preprocessing import scale \n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.random_projection import GaussianRandomProjection, SparseRandomProjection\n",
    "from scipy.stats import sem\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_metrics(y_true,y_pred,prob):\n",
    "    print('Accuracy:', round(metrics.accuracy_score(y_true,y_pred),3))\n",
    "    print('Precision:', round(metrics.precision_score(y_true, y_pred),3))\n",
    "    print('Sensitivity:', round(metrics.recall_score(y_true,y_pred),3))\n",
    "    temp_tn, temp_fp, temp_fn, temp_tp = metrics.confusion_matrix(y_true,y_pred).ravel()\n",
    "    temp_specificity = temp_tn / (temp_tn + temp_fp)\n",
    "    print('Specificity:', round(temp_specificity,3))\n",
    "    print('F1-score:',round(metrics.f1_score(y_true,y_pred),3))\n",
    "    print('AUC: ', round(metrics.roc_auc_score(y_true, prob),3))\n",
    "    print('MCC: ', round(metrics.matthews_corrcoef(y_true,y_pred),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build features for independent test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildFeatures(ampfile, nonampfile ,out, split1_prop, split2_prop):\n",
    "    '''\n",
    "    Function to build required features by calling on the R script \n",
    "        \n",
    "    Parameters:\n",
    "      ampfile: Path to a fasta format AMP file, or a text file in fasta format\n",
    "      non-ampfile: Path to a fasta format Non-AMP file, or a text file in fasta format\n",
    "      out: Name of output file\n",
    "      split1_prop: First split (left-side) fraction of the peptide sequence \n",
    "      split2_prop: Second split (middle) fraction of the peptide sequence\n",
    "      \n",
    "      *split1/2_prop must be less than 1\n",
    "    Returns:\n",
    "      Write the feature matrix to the 'features' folder\n",
    "    '''\n",
    "    \n",
    "    # Command to execute\n",
    "    command = f\"Rscript buildFeatures.R {ampfile} {nonampfile} \\\n",
    "                    {out} {split1_prop} {split2_prop}\"\n",
    "    process = subprocess.Popen(command, stdout=subprocess.PIPE, shell=True)\n",
    "    # Print output\n",
    "    for line in process.stdout:\n",
    "        print(line.decode(\"utf-8\"),end='')\n",
    "    # Wait for the command to finish\n",
    "    process.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing done!\n",
      "#########AMP-Sequence Summary############\n",
      "984 peptide sequences are found after preprocessing!\n",
      "Minimum sequence length is 10 \n",
      "Maximum sequence length is 255 \n",
      "Average sequence length is 44.5437 \n",
      "##########################################\n",
      "#########NonAMP-Sequence Summary############\n",
      "984 peptide sequences are found after preprocessing!\n",
      "Minimum sequence length is 10 \n",
      "Maximum sequence length is 94 \n",
      "Average sequence length is 29.9685 \n",
      "##########################################\n",
      "AAC done!\n",
      "PHYC done!\n",
      "PAAC done!\n",
      "NAAC done!\n",
      "SAAC done!\n",
      "Finished!\n",
      "Feature matrix stored in feature_matrix/iAMPpred_features.csv \n"
     ]
    }
   ],
   "source": [
    "## build the feature matrix for the training data \n",
    "buildFeatures('Training_Dataset/iAMPpred_pos_fasta.txt','Training_Dataset/iAMPpred_neg_fasta.txt',\\\n",
    "             'feature_matrix/iAMPpred_features.csv',0.2,0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing done!\n",
      "#########AMP-Sequence Summary############\n",
      "39 peptide sequences are found after preprocessing!\n",
      "Minimum sequence length is 12 \n",
      "Maximum sequence length is 456 \n",
      "Average sequence length is 78.07692 \n",
      "##########################################\n",
      "#########NonAMP-Sequence Summary############\n",
      "894 peptide sequences are found after preprocessing!\n",
      "Minimum sequence length is 52 \n",
      "Maximum sequence length is 500 \n",
      "Average sequence length is 284.868 \n",
      "##########################################\n",
      "AAC done!\n",
      "PHYC done!\n",
      "PAAC done!\n",
      "NAAC done!\n",
      "SAAC done!\n",
      "Finished!\n",
      "Feature matrix stored in feature_matrix/dbAMP_human_features.csv \n"
     ]
    }
   ],
   "source": [
    "## build the feature matrix for the independent testing data\n",
    "buildFeatures('Independent_set/Independent_Dataset/human_Positive.txt','Independent_set/Independent_Dataset/human_Negative.txt',\n",
    "             'feature_matrix/dbAMP_human_features.csv',0.2,0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare independent testing data\n",
    "X_test = pd.read_csv('feature_matrix/dbAMP_human_features.csv',index_col=0)\n",
    "y_test = X_test.pop('labels')\n",
    "\n",
    "## Prepare training data\n",
    "X_train = pd.read_csv('feature_matrix/iAMPpred_features.csv',index_col=0)\n",
    "y_train = X_train.pop('labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Projection based model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_ensemble(scores,y_tr,dim,Runs):\n",
    "    '''\n",
    "    Function to ensemble scores across RP runs \n",
    "        \n",
    "    Parameters:\n",
    "      scores: prediction score matrix\n",
    "      y_tr: actual labels\n",
    "      Runs: number of RP runs\n",
    "      \n",
    "    Returns:\n",
    "      None. Print out the results\n",
    "    '''\n",
    "    # average the score across all runs of RP\n",
    "    predictions_res = pd.DataFrame(np.mean(scores,axis=1))\n",
    "\n",
    "    # score > 0 will be 1, < 0 will be 0\n",
    "    y_pred = (predictions_res[0]>0).astype(int)\n",
    "    \n",
    "    # evaluation metrics\n",
    "    acc = metrics.accuracy_score(y_tr,y_pred)\n",
    "    recall = metrics.recall_score(y_tr,y_pred) \n",
    "    tn, fp, fn, tp = metrics.confusion_matrix(y_tr, y_pred).ravel()\n",
    "    specificity = tn / (tn+fp)\n",
    "    precision = metrics.precision_score(y_tr, y_pred)\n",
    "    auc = metrics.roc_auc_score(y_tr, predictions_res[0])\n",
    "    f1 = metrics.f1_score(y_tr,y_pred)\n",
    "    \n",
    "    # below is to find the std across all runs\n",
    "    temp_acc = []\n",
    "    temp_sn = []\n",
    "    temp_sp = []\n",
    "    temp_auc = []\n",
    "    temp_prec = []\n",
    "    temp_f1 = []\n",
    "    \n",
    "    # loop through all runs and calculate metrics for each run\n",
    "    for i in range(Runs):\n",
    "        y_pred_perrun = (scores[f'Run_{i}'] >0).astype(int)\n",
    "\n",
    "        temp_tn, temp_fp, temp_fn, temp_tp = \\\n",
    "            metrics.confusion_matrix(y_tr, y_pred_perrun).ravel()\n",
    "        \n",
    "        temp_acc.append(metrics.accuracy_score(y_tr,y_pred_perrun))\n",
    "        temp_sn.append(metrics.recall_score(y_tr,y_pred_perrun))\n",
    "        temp_sp.append(temp_tn / (temp_tn + temp_fp))\n",
    "        temp_auc.append(metrics.roc_auc_score(y_tr, scores[f'Run_{i}']))\n",
    "        temp_prec.append(metrics.precision_score(y_tr, y_pred_perrun))\n",
    "        temp_f1.append(metrics.f1_score(y_tr,y_pred_perrun))\n",
    "        \n",
    "    std_acc = np.std(temp_acc)\n",
    "    std_sn = np.std(temp_sn)\n",
    "    std_sp = np.std(temp_sp)\n",
    "    std_auc = np.std(temp_auc)\n",
    "    std_prec = np.std(temp_prec)\n",
    "    std_f1 = np.std(temp_f1)\n",
    "    \n",
    "    print(f\"Dimension {dim} =>> \\n\",\"Accuracy : %.2f\" % (acc*100),'±',\"%.3f \\n\" % std_acc,\n",
    "          \" Sensitivity : %.2f\" % (recall*100),'±',\"%.3f \\n\" % std_sn, \n",
    "         \" Specificity : %.2f\" % (specificity*100),'±',\"%.3f \\n\" % std_sp,\n",
    "         \" Precision : %.2f\" % (precision*100),'±',\"%.3f \\n\" % std_prec,\n",
    "         \" AUC : %.2f\" % (auc*100),'±',\"%.3f \\n\" % std_auc,\n",
    "         \" F1 : %.2f\" % (f1*100),'±',\"%.3f \\n\" % std_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RP_train(X,y,dim,method='Gaussian',Runs=10):    \n",
    "    '''\n",
    "    Function to train an SVM classifier with \\\n",
    "        random projection-based dimensionality reduction\n",
    "        \n",
    "    Parameters:\n",
    "      X: Input feature matrix for training the model\n",
    "      y: AMP/Non_AMP labels for training the model\n",
    "      dim: Desired dimensionality of the projected space\n",
    "      method: Random projection method to use (default: 'Gaussian')\n",
    "      Runs: Number of runs to perform RP (default: 10)\n",
    "    Returns:\n",
    "      None (prints evaluation metrics)\n",
    "    '''\n",
    "    scores = pd.DataFrame()\n",
    "    for i in range(Runs):\n",
    "        \n",
    "        if method == 'Gaussian':\n",
    "            transformer = \\\n",
    "            GaussianRandomProjection(random_state=i,n_components=dim)\n",
    "        elif method == 'Sparse':\n",
    "            transformer = \\\n",
    "            SparseRandomProjection(random_state=i, n_components=dim)\n",
    "        \n",
    "        Xtr = transformer.fit_transform(X)\n",
    "        ytr = np.array(y).ravel()\n",
    "\n",
    "        ## Define model\n",
    "        svm_model = SVC(random_state=1)\n",
    "        ## Define the parameter grid for grid search\n",
    "        param_grid = {'C': [0.1, 1, 10, 100], \n",
    "                      'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
    "                      'kernel': ['rbf']} \n",
    "\n",
    "        grid_search = GridSearchCV(svm_model, param_grid, \\\n",
    "                                   cv=10,n_jobs=-1,scoring='accuracy')\n",
    "        grid_search.fit(Xtr, ytr)\n",
    "        \n",
    "        # define a new classifier with the best hyperparameters\n",
    "        clf = SVC(**grid_search.best_params_,random_state=1) \n",
    "\n",
    "        # use kfold CV to evaluate model performance \n",
    "        kfold = KFold(n_splits=10, shuffle=True,random_state=i)\n",
    "        \n",
    "        # predict each class with a score using CV\n",
    "        score = cross_val_predict(clf, Xtr, ytr, \\\n",
    "                                  cv=kfold, method='decision_function')\n",
    "        # store the score for each run of RP\n",
    "        score = pd.DataFrame(score)\n",
    "        score.columns = [f'Run_{i}']\n",
    "        scores = pd.concat([scores,score],axis=1)\n",
    "    \n",
    "    score_ensemble(scores,ytr,dim,Runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension 50 =>> \n",
      " Accuracy : 93.09 ± 0.005 \n",
      "  Sensitivity : 90.55 ± 0.004 \n",
      "  Specificity : 95.63 ± 0.011 \n",
      "  Precision : 95.40 ± 0.010 \n",
      "  AUC : 97.63 ± 0.002 \n",
      "  F1 : 92.91 ± 0.005 \n",
      "\n",
      "Dimension 100 =>> \n",
      " Accuracy : 93.60 ± 0.005 \n",
      "  Sensitivity : 91.97 ± 0.007 \n",
      "  Specificity : 95.22 ± 0.007 \n",
      "  Precision : 95.06 ± 0.007 \n",
      "  AUC : 97.87 ± 0.002 \n",
      "  F1 : 93.49 ± 0.005 \n",
      "\n",
      "Dimension 150 =>> \n",
      " Accuracy : 93.65 ± 0.004 \n",
      "  Sensitivity : 91.97 ± 0.007 \n",
      "  Specificity : 95.33 ± 0.005 \n",
      "  Precision : 95.16 ± 0.005 \n",
      "  AUC : 97.82 ± 0.002 \n",
      "  F1 : 93.54 ± 0.004 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in [50,100,150]:\n",
    "    RP_train(X_train,y_train,dim = i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Projection based model testing on independent testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Random Projection for independent test\n",
    "def RP_indTest(trainX,trainy,testX,testy,dim,method='Gaussian',Runs=10):\n",
    "    '''\n",
    "    Similar to the code above\n",
    "    '''\n",
    "    scores = pd.DataFrame()\n",
    "    for i in range(Runs):\n",
    "        ## RP\n",
    "        if method == 'Gaussian':\n",
    "            transformer = GaussianRandomProjection(random_state=i,n_components=dim)\n",
    "        elif method == 'Sparse':\n",
    "            transformer = SparseRandomProjection(random_state=i, n_components=dim)\n",
    "        \n",
    "        Xtt = transformer.fit_transform(testX)\n",
    "        ytt = np.array(testy).ravel()\n",
    "        \n",
    "        Xtr = transformer.fit_transform(trainX)\n",
    "        ytr = np.array(trainy).ravel()\n",
    "        \n",
    "        ## Define model\n",
    "        svm_model = SVC(random_state=1)\n",
    "        # Define the parameter grid for grid search\n",
    "        param_grid = {'C': [0.1, 1, 10, 100], \n",
    "                      'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
    "                      'kernel': ['rbf']} \n",
    "\n",
    "        grid_search = GridSearchCV(svm_model, param_grid, cv=10,n_jobs=-1,scoring='accuracy')\n",
    "        grid_search.fit(Xtr, ytr)\n",
    "        \n",
    "        # use best hyperparameters\n",
    "        clf = SVC(**grid_search.best_params_,random_state=1) \n",
    "        \n",
    "        clf.fit(Xtr,ytr)\n",
    "        # predict each class with a score\n",
    "        score = clf.decision_function(Xtt)## predict on independent test data\n",
    "        score = pd.DataFrame(score)\n",
    "        score.columns = [f'Run_{i}']\n",
    "        scores = pd.concat([scores,score],axis=1)\n",
    "    \n",
    "    score_ensemble(scores,ytt,dim,Runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dbAMP Human Dataset Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension 50 =>> \n",
      " Accuracy : 66.13 ± 0.014 \n",
      "  Sensitivity : 87.18 ± 0.028 \n",
      "  Specificity : 65.21 ± 0.015 \n",
      "  Precision : 9.86 ± 0.005 \n",
      "  AUC : 70.70 ± 0.027 \n",
      "  F1 : 17.71 ± 0.008 \n",
      "\n",
      "Dimension 100 =>> \n",
      " Accuracy : 66.88 ± 0.016 \n",
      "  Sensitivity : 87.18 ± 0.062 \n",
      "  Specificity : 66.00 ± 0.017 \n",
      "  Precision : 10.06 ± 0.007 \n",
      "  AUC : 72.03 ± 0.027 \n",
      "  F1 : 18.04 ± 0.013 \n",
      "\n",
      "Dimension 150 =>> \n",
      " Accuracy : 66.13 ± 0.014 \n",
      "  Sensitivity : 87.18 ± 0.035 \n",
      "  Specificity : 65.21 ± 0.015 \n",
      "  Precision : 9.86 ± 0.005 \n",
      "  AUC : 71.77 ± 0.017 \n",
      "  F1 : 17.71 ± 0.008 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in [50,100,150]:\n",
    "    RP_indTest(X_train,y_train,X_test,y_test,i,Runs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM-based model training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X,y,feature_names):\n",
    "    Xdt = np.array(X) \n",
    "    ydt = np.array(y).ravel()\n",
    "\n",
    "    ## Use all data for grid search\n",
    "    svm_model = SVC()\n",
    "\n",
    "    # Define the parameter grid for grid search\n",
    "    param_grid = {'C': [0.1, 1, 10, 100, 1000], \n",
    "                  'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
    "                  'kernel': ['rbf','sigmoid']} \n",
    "\n",
    "    # Perform grid search with 10 cross-validation\n",
    "    grid_search = GridSearchCV(svm_model, param_grid, cv=10,n_jobs=-1,scoring='accuracy')\n",
    "\n",
    "    # Fit the model with grid search\n",
    "    grid_search.fit(Xdt, ydt)\n",
    "    print(f'Training on {feature_names}!')\n",
    "    print(f\"Best parameters found: \", grid_search.best_params_)\n",
    "    print(f\"Best score found: \", grid_search.best_score_)\n",
    "    \n",
    "    ## Repeat 10 fold cv for 10 times\n",
    "    rkf = RepeatedKFold(n_splits=10, n_repeats=10, random_state=1)\n",
    "    \n",
    "    # use best hyperparameters\n",
    "    clf = SVC(**grid_search.best_params_,random_state=1) \n",
    "    \n",
    "    # evaluation metrics and print them out\n",
    "    accuracy_scores = cross_val_score(clf , Xdt, ydt, scoring='accuracy', cv=rkf, n_jobs=-1)\n",
    "    f1_scores = cross_val_score(clf , Xdt, ydt, scoring='f1', cv=rkf, n_jobs=-1)\n",
    "    precision = cross_val_score(clf , Xdt, ydt, scoring='precision', cv=rkf, n_jobs=-1)\n",
    "    recall = cross_val_score(clf , Xdt, ydt, scoring='recall', cv=rkf, n_jobs=-1)\n",
    "\n",
    "    specificity = make_scorer(recall_score, pos_label=0)\n",
    "    specificity = cross_val_score(clf, Xdt, ydt, cv=rkf, scoring = specificity,n_jobs=-1)\n",
    "    \n",
    "    auc = cross_val_score(clf, Xdt, ydt, cv=rkf, scoring = 'roc_auc',n_jobs=-1)\n",
    "    \n",
    "    print(f'Accuracy:', round(accuracy_scores.mean()*100,2),'±'\n",
    "          ,round(sem(accuracy_scores*100),3))\n",
    "    print(f'F1-scores:', round(f1_scores.mean()*100,2),'±'\n",
    "          ,round(sem(f1_scores*100),3))\n",
    "    print(f'Precision:', round(precision.mean()*100,2),'±'\n",
    "          ,round(sem(precision*100),3))\n",
    "    print(f'Sensitivity:', round(recall.mean()*100,2),'±'\n",
    "          ,round(sem(recall*100),3))\n",
    "    print(f'Specificity:', round(specificity.mean()*100,2),'±'\n",
    "          ,round(sem(specificity*100),3))\n",
    "    print(f'{feature_names} AUC:', round(auc.mean()*100,2),'±'\n",
    "          ,round(sem(auc*100),3))\n",
    "    print(f'Training done!')\n",
    "    print('########################')\n",
    "    \n",
    "    return clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## iAMPpred "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on iAMPpred training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on all features!\n",
      "Best parameters found:  {'C': 1, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "Best score found:  0.9329068683310888\n",
      "Accuracy: 93.33 ± 0.169\n",
      "F1-scores: 93.18 ± 0.179\n",
      "Precision: 94.77 ± 0.245\n",
      "Sensitivity: 91.71 ± 0.278\n",
      "Specificity: 94.97 ± 0.225\n",
      "all features AUC: 97.75 ± 0.084\n",
      "Training done!\n",
      "########################\n"
     ]
    }
   ],
   "source": [
    "clf = model(X_train,y_train,'all features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting on independent human dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction results for human dataset\n",
      "Accuracy: 0.636\n",
      "Precision: 0.094\n",
      "Sensitivity: 0.897\n",
      "Specificity: 0.624\n",
      "F1-score: 0.171\n",
      "AUC:  0.711\n",
      "MCC:  0.213\n"
     ]
    }
   ],
   "source": [
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "prob = clf.decision_function(X_test)\n",
    "print('Prediction results for human dataset')\n",
    "eval_metrics(y_test,y_pred,prob)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
